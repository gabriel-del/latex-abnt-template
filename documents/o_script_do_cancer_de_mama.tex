\chapter{O script do cancer de mama}
\label{chapter:o_script_do_cancer_de_mama}

O script começa com a importação da biblioteca “Pandas”, definida anteriormente, na primeira linha e em seguida vários datasets do Scikit-learn: 

  KNeighborsClassifier, LigisticRegression, cross\_val\_score, DecisionTreeClassifier, RandomForestClassifier,        GaussianNB, MLPClassifier. 

Esses atributos serão utilizados posteriormente, para a execução do script, porém sua importação é de extrema importância.


\begin{lstlisting}[language=Python, caption=Importação de bibliotecas]
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
\end{lstlisting}

Em seguida há uma configuração para melhorar a visualização do script, onde há a retirada de avisos indesejados, que possam vir distorcer a visualização.

\begin{lstlisting}[language=Python, caption=Ignorar Avisos]
import warnings
warnings.filterwarnings('ignore')
\end{lstlisting}

Declaração da lista e ler o arquivo data.csv do banco de dados do câncer de Mama o qual será analisado.
\begin{lstlisting}[language=Python, caption=Ler arquivo data.csv]
lista = []
cancer = pd.read_csv('./data.csv', index_col=0)
\end{lstlisting}

Separação em matrizes “X” e “Y”, com o “Y” sendo o diagnóstico e o “X” os atributos. E início do loop que permitirá à procura das informações da base para realizar a análise.
\begin{lstlisting}[language=Python, caption=Separar matrizes]
diag = {'M':0, 'B':1}
cancer.diagnosis = [diag[item] for item in cancer.diagnosis]
X = cancer.as_matrix(cancer.columns[1:31])
y = cancer.as_matrix(['diagnosis'])
\end{lstlisting}



Então as funções responsáveis pela análise dos atributos:
K-Nearest Neighbors:
\begin{lstlisting}[language=Python, caption=KNeighborsClassifier]
knn = KNeighborsClassifier(n_neighbors = 5, weights='uniform')
scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
lista.append([scores.mean(),scores.std()])
\end{lstlisting}

Decision Tree:
\begin{lstlisting}[language=Python, caption=DecisionTreeClassifier]
tree = DecisionTreeClassifier(max_depth=3, random_state=0)
scores = cross_val_score(tree, X, y, cv=5, scoring='accuracy')
lista.append([scores.mean(),scores.std()])
\end{lstlisting}

Randon Forest:
\begin{lstlisting}[language=Python, caption=RandomForesClassifier]
forest = RandomForestClassifier(n_estimators=50, random_state=0)
scores = cross_val_score(forest, X, y, cv=5, scoring='accuracy')
\end{lstlisting}

Suport Vector Machine:
\begin{lstlisting}[language=Python, caption=SVC]
svm = SVC(kernel='poly',degree=1)
scores = cross_val_score(svm, X, y, cv=5, scoring='accuracy')
\end{lstlisting}
Naive Bayes:
\begin{lstlisting}[language=Python, caption=GaussianNB]
gnb = GaussianNB()
scores = cross_val_score(gnb, X, y, cv=5, scoring='accuracy')
\end{lstlisting}

Artificial Neural Network:
\begin{lstlisting}[language=Python, caption=MLPClassifier]
mlp = MLPClassifier(solver='adam', alpha=0.0001, hidden_layer_sizes=(10,20,40), random_state=42, learning_rate='constant', learning_rate_init=0.01, max_iter=100, activation='logistic', momentum=0.9, tol=0.0001)
scores = cross_val_score(mlp, X, y, cv=5, scoring='accuracy')
\end{lstlisting}

Finalizando tem-se as informações de saída, utilizadas para o diagnóstico e checar a eficácia do Script:
\begin{lstlisting}[language=Python, caption=Média e Desvio]
print('Media:', scores.mean())
print('Desvio:', scores.std())
\end{lstlisting}

Cria-se uma tabela com os valores obtidos:
\begin{lstlisting}[language=Python, caption=Resultados]
df = pd.DataFrame(lista, columns = ["Acuracia","Variancia"])
df.to_csv(r'resultados.csv')
\end{lstlisting}

A junção de cada parte de cada etapa do script trouxe a seguinte resolução:
\begin{lstlisting}[language=Python, caption=Código completo]
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
import warnings
warnings.filterwarnings('ignore')
lista = []

cancer = pd.read_csv('./data.csv', index_col=0)

diag = {'M':0, 'B':1}
cancer.diagnosis = [diag[item] for item in cancer.diagnosis]
X = cancer.as_matrix(cancer.columns[1:31])
y = cancer.as_matrix(['diagnosis'])
knn = KNeighborsClassifier(n_neighbors = 1, weights='uniform')
scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
lista.append([scores.mean(),scores.std()])

tree = DecisionTreeClassifier(max_depth=3, random_state=0)
scores = cross_val_score(tree, X, y, cv=5, scoring='accuracy')
lista.append([scores.mean(),scores.std()])

forest = RandomForestClassifier(n_estimators=50, random_state=0)
scores = cross_val_score(forest, X, y, cv=5, scoring='accuracy')

svm = SVC(kernel='poly',degree=1)
scores = cross_val_score(svm, X, y, cv=5, scoring='accuracy')

gnb = GaussianNB()
scores = cross_val_score(gnb, X, y, cv=5, scoring='accuracy')

mlp = MLPClassifier(solver='adam', alpha=0.0001, hidden_layer_sizes=(10,20,40), random_state=42, learning_rate='constant', learning_rate_init=0.01, max_iter=100, activation='logistic', momentum=0.9, tol=0.0001)
scores = cross_val_score(mlp, X, y, cv=5, scoring='accuracy')

print('Media:', scores.mean())
print('Desvio:', scores.std())

df = pd.DataFrame(lista, columns = ["Acuracia","Variancia"])
df.to_csv(r'resultados.csv')
\end{lstlisting}

Por fim, o Script conseguiu executar os comandos juntamente com as funções demonstradas anteriormente, e apresentou como resultados, após testes, uma acurácia de 88\% e um desvio padrão de 2\% na detecção e análise de tumores malignos de câncer.



